import os
import sys
import json
import logging
import signal
import asyncio
import concurrent.futures
import uuid
from typing import List, Dict, Tuple, Optional
from functools import partial

import numpy as np
import cv2
import redis
import aiohttp

# OpenCV ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (ThreadPoolExecutorì™€ì˜ ê²½í•© ë°©ì§€)
cv2.setNumThreads(2)

# --- ê²½ë¡œ ì„¤ì • ---
WORKER_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, WORKER_DIR)

# --- ì‹ ê·œ íŒŒì´í”„ë¼ì¸ ë° ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from inpainting_pipeline.image_inpainter import ImageInpainter
from ocr_pipeline.worker import OcrProcessor
from core.config import (
    LOG_LEVEL,
    WORKER_COLLECT_BATCH_SIZE,
    INPAINTER_GPU_BATCH_SIZE,
    WORKER_BATCH_MAX_WAIT_TIME_SECONDS,
    PROCESSOR_TASK_QUEUE,
    CPU_WORKER_COUNT,
    JPEG_QUALITY,
    MAX_CONCURRENT_TASKS,
    MAX_PENDING_TASKS,
    SHUTDOWN_MAX_WAIT_SECONDS
)
from core.redis_client import initialize_redis, close_redis, get_redis_client, enqueue_error_result, enqueue_success_result, set_task_completion_callback
from core.image_downloader import download_image_async
from dispatching_pipeline.mask import filter_chinese_ocr_result, generate_mask_pure_sync
from dispatching_pipeline.text_translate import process_and_save_translation
from dispatching_pipeline.resize_handler import handle_no_chinese_text_sync
from hosting.r2hosting import R2ImageHosting
from rendering_pipeline.result_check import ResultChecker
from rendering_pipeline.rendering import RenderingProcessor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logging.getLogger('asyncio').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# === ì „ì—­ ì‘ì—… ì¹´ìš´í„° (ì„œë²„ ì•ˆì •í™”ìš©) ===
pending_tasks_count_8473 = 0
tasks_count_lock_8473 = asyncio.Lock()

async def _decrease_pending_count():
    """ì „ì—­ ì‘ì—… ì¹´ìš´í„°ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤"""
    global pending_tasks_count_8473
    async with tasks_count_lock_8473:
        pending_tasks_count_8473 -= 1
    logger.debug(f"Task completed, pending count: {pending_tasks_count_8473}")

class AsyncInpaintingWorker:
    """inpainting_pipelineê³¼ ë‚´ë¶€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” í†µí•© ë¹„ë™ê¸° ì›Œì»¤"""
    
    def __init__(self):
        self.cpu_executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=CPU_WORKER_COUNT, thread_name_prefix="cpu-worker"
        )
        self.gpu_executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=1, thread_name_prefix="gpu-worker"
        )
        self.concurrent_task_semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
        
        self.inpainter = ImageInpainter(executor=self.cpu_executor)
        self.ocr_processor: Optional[OcrProcessor] = None
        
        self.batch_lock = asyncio.Lock()
        self.task_batch: List[Tuple[np.ndarray, np.ndarray, Dict, np.ndarray]] = []
        self.batch_trigger = asyncio.Event()

        self.r2_hosting = R2ImageHosting()
        self.http_session: Optional[aiohttp.ClientSession] = None
        self._running = False
        self._workers: List[asyncio.Task] = []
        self.rendering_processor: Optional[RenderingProcessor] = None
        self.result_checker: Optional[ResultChecker] = None

    async def start_workers(self):
        self._running = True
        self.http_session = aiohttp.ClientSession()
        main_loop = asyncio.get_running_loop()

        self.ocr_processor = OcrProcessor(
            loop=main_loop, 
            cpu_executor=self.cpu_executor,
            gpu_executor=self.gpu_executor,
            jpeg_quality=JPEG_QUALITY
        )
        await self.ocr_processor.initialize_model()
        logger.info("âœ… OCR Processor initialized.")

        self.rendering_processor = RenderingProcessor(loop=main_loop)
        self.result_checker = ResultChecker(
            cpu_executor=self.cpu_executor,
            rendering_processor=self.rendering_processor,
            http_session=self.http_session
        )
        logger.info("âœ… Rendering modules initialized.")

        redis_listener = asyncio.create_task(self._redis_listener_worker("redis-listener"))
        batch_processor = asyncio.create_task(self._inpainting_batch_processor("inpainting-processor"))
        self._workers = [redis_listener, batch_processor]
        logger.info(f"ğŸš€ Started {len(self._workers)} main workers.")

    async def stop_workers(self):
        self._running = False
        if self.inpainter:
            self.inpainter.close()
        if self.ocr_processor:
            self.ocr_processor.close()
        
        for worker in self._workers:
            worker.cancel()
        await asyncio.gather(*self._workers, return_exceptions=True)
        
        if self.http_session:
            await self.http_session.close()
        
        self.cpu_executor.shutdown(wait=True)
        self.gpu_executor.shutdown(wait=True)
        logger.info("Stopped all workers and shut down thread pools.")

    async def run_cpu_task(self, func, *args, **kwargs):
        return await asyncio.get_running_loop().run_in_executor(
            self.cpu_executor, partial(func, *args, **kwargs)
        )

    async def process_task_from_redis(self, task_data: dict):
        """Redisì—ì„œ ë°›ì€ ì‘ì—…ì„ ì²˜ë¦¬í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰"""
        # request_idê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ task_dataì— ì¶”ê°€
        request_id = task_data.get("request_id")
        if not request_id:
            request_id = str(uuid.uuid4())
            task_data["request_id"] = request_id
            logger.info(f"Generated new request_id: {request_id} for image_id: {task_data.get('image_id')}")
            
        image_id = task_data.get("image_id")
        try:
            image_url = task_data.get("image_url")
            
            # 1. OCR ì‹¤í–‰ (ë¦¬íŒ©í† ë§ìœ¼ë¡œ ë¡œì§ ìœ„ì¹˜ ì´ë™)
            image_bytes = await download_image_async(self.http_session, image_url, request_id)
            if image_bytes is None:
                await enqueue_error_result(request_id, image_id, "Image download failed")
                return

            ocr_result = await self.ocr_processor.process_image(image_bytes, image_id, request_id)
            
            # 2. ì›ë˜ì˜ task_data êµ¬ì¡°ë¥¼ ì™„ë²½í•˜ê²Œ ì¬í˜„
            task_data['ocr_result'] = ocr_result

            # 3. ì—¬ê¸°ë¶€í„°ëŠ” ì›ë˜ operate_workerì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‹¤í–‰
            is_long = task_data.get("is_long", False)
            filtered_ocr = filter_chinese_ocr_result(task_data.get("ocr_result") or [], request_id)

            if not filtered_ocr:
                final_url = await self.run_cpu_task(
                    handle_no_chinese_text_sync, 
                    image_bytes, 
                    image_url, 
                    request_id, 
                    image_id, 
                    is_long,
                    self.r2_hosting
                )
                await enqueue_success_result(request_id, image_id, final_url or image_url)
                return
                
            # ë§ˆìŠ¤í¬ ìƒì„± (ë˜í¼ í•¨ìˆ˜ ì—†ì´ ì§ì ‘ í˜¸ì¶œ)
            mask_gen_result = await self.run_cpu_task(
                generate_mask_pure_sync, image_bytes, filtered_ocr, request_id
            )
            image_array, mask_array = mask_gen_result

            # ë²ˆì—­ ì‘ì—… (ê¸°ì¡´ ì½”ë“œì™€ ê°™ì´ filtered_ocrì„ task_dataì— ì¶”ê°€)
            task_data["filtered_ocr_result"] = filtered_ocr
            await process_and_save_translation(task_data, image_url, self.result_checker)

            # ì¸í˜ì¸íŒ… ë°°ì¹˜ì— ì¶”ê°€
            task_info = {"request_id": request_id, "image_id": image_id, "is_long": is_long}
            async with self.batch_lock:
                self.task_batch.append((image_array, mask_array, task_info, image_array))
            
            if len(self.task_batch) >= WORKER_COLLECT_BATCH_SIZE:
                self.batch_trigger.set()
                
        except Exception as e:
            logger.error(f"[{request_id}] Error in task processing: {e}", exc_info=True)
            await enqueue_error_result(request_id, image_id, f"Task processing error: {str(e)}")
        finally:
            # ì˜ˆì™¸ê°€ ë°œìƒí•˜ë”ë¼ë„ ì¹´ìš´í„°ëŠ” ê°ì†Œë˜ì–´ì•¼ í•¨ (ë‹¨, enqueue_error_resultì—ì„œ ì´ë¯¸ ê°ì†Œì‹œí‚¨ ê²½ìš° ì œì™¸)
            # í•˜ì§€ë§Œ enqueue í•¨ìˆ˜ë“¤ì´ ì´ë¯¸ ì¹´ìš´í„°ë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” semaphoreë§Œ í•´ì œ
            self.concurrent_task_semaphore.release()

    async def _redis_listener_worker(self, name: str):
        """Redisì—ì„œ ì‘ì—…ì„ ê°€ì ¸ì™€ ì²˜ë¦¬"""
        global pending_tasks_count_8473
        logger.info(f"Worker '{name}' started, listening on '{PROCESSOR_TASK_QUEUE}'.")
        while self._running:
            try:
                # ëŒ€ê¸° ì‘ì—… ìˆ˜ê°€ ìµœëŒ€ì¹˜ë¥¼ ë„˜ìœ¼ë©´ ìµœëŒ€ì¹˜ ì•„ë˜ë¡œ ë‚´ë ¤ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
                while True:
                    async with tasks_count_lock_8473:
                        current_pending = pending_tasks_count_8473
                    if current_pending < MAX_PENDING_TASKS:
                        break
                    logger.warning(f"Too many pending tasks ({current_pending}), waiting for completion...")
                    await asyncio.sleep(1)
                    if not self._running:
                        return

                await self.concurrent_task_semaphore.acquire()
                try:
                    # timeout=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì‘ì—…ì´ ì˜¬ ë•Œê¹Œì§€ ë¬´í•œì • ëŒ€ê¸° (ê°€ì¥ íš¨ìœ¨ì )
                    task_tuple = await get_redis_client().blpop([PROCESSOR_TASK_QUEUE], timeout=0)

                    # blpop(timeout=0)ì€ í•­ìƒ ê°’ì„ ë°˜í™˜í•˜ë¯€ë¡œ task_tupleì€ Noneì´ ë  ìˆ˜ ì—†ìŒ
                    async with tasks_count_lock_8473:
                        pending_tasks_count_8473 += 1
                    logger.debug(f"Task received, pending count: {pending_tasks_count_8473}")

                    task_data = json.loads(task_tuple[1].decode('utf-8'))
                    asyncio.create_task(self.process_task_from_redis(task_data))

                except (redis.exceptions.RedisError, json.JSONDecodeError) as e:
                    logger.error(f"Error in '{name}': {e}", exc_info=True)
                    self.concurrent_task_semaphore.release()
                    await asyncio.sleep(5)
                except asyncio.CancelledError:
                    # ì‘ì—…ì ì¢…ë£Œ ì‹œ ëŒ€ê¸° ì¤‘ì´ë˜ semaphoreë¥¼ í•´ì œ
                    self.concurrent_task_semaphore.release()
                    break
            except Exception as e:
                # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ë°œìƒ ì‹œ semaphore ëˆ„ìˆ˜ ë°©ì§€
                if self.concurrent_task_semaphore.locked():
                    self.concurrent_task_semaphore.release()
                logger.critical(f"Unexpected error in '{name}': {e}", exc_info=True)
                await asyncio.sleep(5)

    async def _inpainting_batch_processor(self, name: str):
        """ì£¼ê¸°ì ìœ¼ë¡œ ë˜ëŠ” íŠ¸ë¦¬ê±°ì— ì˜í•´ ê¹¨ì–´ë‚˜ ë°°ì¹˜ ì²˜ë¦¬"""
        logger.info(f"Worker '{name}' started.")
        while self._running:
            try:
                await asyncio.wait_for(self.batch_trigger.wait(), timeout=WORKER_BATCH_MAX_WAIT_TIME_SECONDS)
            except asyncio.TimeoutError:
                pass
            finally:
                self.batch_trigger.clear()
            
            async with self.batch_lock:
                if not self.task_batch:
                    continue
                batch_to_process = list(self.task_batch)
                self.task_batch.clear()
            
            await self._process_batch(batch_to_process)

    async def _process_batch(self, batch: List[Tuple[np.ndarray, np.ndarray, Dict, np.ndarray]]):
        if not batch: return

        images, masks, tasks_info, original_image_arrays = zip(*batch)
        request_ids = [info['request_id'] for info in tasks_info]
        logger.info(f"Processing batch of {len(batch)} tasks. IDs: {request_ids}")

        try:
            results_iterator = self.inpainter.process_images(
                image_list=list(images), mask_list=list(masks),
                batch_size=INPAINTER_GPU_BATCH_SIZE
            )
            
            result_tasks = [
                self._handle_inpainting_result(tasks_info[idx], result_img, original_image_arrays[idx])
                for idx, result_img in results_iterator
            ]
            await asyncio.gather(*result_tasks)

        except Exception as e:
            logger.error(f"Failed to process inpainting batch: {e}", exc_info=True)
            error_tasks = [
                enqueue_error_result(task['request_id'], task['image_id'], "Inpainting batch failed")
                for task in tasks_info
            ]
            await asyncio.gather(*error_tasks)

    async def _handle_inpainting_result(self, task_info: Dict, result_image: np.ndarray, original_image_array: np.ndarray):
        """ì¸í˜ì¸íŒ… ê²°ê³¼ë¥¼ ResultCheckerë¡œ ì „ë‹¬"""
        request_id = task_info["request_id"]
        try:
            inpainting_data = {
                **task_info, 
                "inpainted_image": result_image,
                "original_image_array": original_image_array
            }
            await self.result_checker.save_inpainting_result(request_id, inpainting_data)
            logger.info(f"[{request_id}] Inpainting result (numpy array) saved and forwarded.")
        except Exception as e:
            logger.error(f"[{request_id}] Failed to handle inpainting result: {e}", exc_info=True)
            await enqueue_error_result(request_id, task_info['image_id'], "Result handling failed")

async def main():
    global pending_tasks_count_8473
    worker = AsyncInpaintingWorker()
    stop_event = asyncio.Event()

    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, stop_event.set)
    
    try:
        await initialize_redis()
        
        # Redis clientì— ì‘ì—… ì™„ë£Œ ì½œë°± ì„¤ì •
        set_task_completion_callback(_decrease_pending_count)
        
        await worker.start_workers()
        logger.info("ğŸš€ Inpainting Worker started successfully.")
        await stop_event.wait()

    finally:
        logger.info("Shutting down workers...")
        
        # Graceful shutdown: ëŒ€ê¸° ì‘ì—…ì´ ìˆìœ¼ë©´ ìµœëŒ€ 100ì´ˆê¹Œì§€ ê¸°ë‹¤ë¦¼
        async with tasks_count_lock_8473:
            current_pending = pending_tasks_count_8473
        
        if current_pending == 0:
            logger.info("No pending tasks, shutting down immediately.")
        else:
            logger.info(f"Waiting for {current_pending} pending tasks to complete (max {SHUTDOWN_MAX_WAIT_SECONDS}s)...")
            
            for i in range(SHUTDOWN_MAX_WAIT_SECONDS):
                async with tasks_count_lock_8473:
                    current_pending = pending_tasks_count_8473
                
                if current_pending == 0:
                    logger.info(f"All pending tasks completed after {i+1}s.")
                    break
                    
                await asyncio.sleep(1)
                
                if (i + 1) % 10 == 0:  # 10ì´ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
                    logger.info(f"Still waiting... {current_pending} tasks pending ({i+1}s elapsed)")
            else:
                logger.warning(f"Shutdown timeout reached. {current_pending} tasks still pending.")
        
        await worker.stop_workers()
        await close_redis()
        logger.info("Worker shutdown complete.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        logger.critical(f"Critical error: {e}", exc_info=True)
        exit(1)
