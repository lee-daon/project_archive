# Lama Inpainting Docker 환경 설정 및 실행 가이드

## Docker 이미지 빌드

프로젝트 루트 디렉토리 (`/c:/Users/leeda/programing/test/`)에서 다음 명령어를 실행하여 Docker 이미지를 빌드합니다. 이미지는 환경 설정과 모델 파일을 포함합니다.

```bash
docker build -t lama-inpainting-env -f build_env/Dockerfile .
```

*   `-t lama-inpainting-env`: 생성될 이미지의 이름을 `lama-inpainting-env`로 지정합니다.
*   `-f build_env/Dockerfile`: 사용할 Dockerfile의 경로를 지정합니다.
*   `.`: Docker 빌드 컨텍스트를 현재 디렉토리로 설정합니다. 이 컨텍스트를 통해 `Dockerfile`에서 `COPY lama-fourier ...` 명령어가 모델 파일을 찾을 수 있습니다.

## Docker 컨테이너 실행

빌드된 이미지를 사용하여 컨테이너를 실행합니다. 이때, 개발 중인 애플리케이션 코드(`lama` 디렉토리)를 컨테이너 내부의 `/app` 경로로 볼륨 마운트하여 실시간 코드 변경사항이 반영되도록 합니다.

```bash
docker run -it --rm --gpus all \
  --name lama-gpu-container \
  -v /c/Users/leeda/programing/test/lama:/app \
  -v /c/Users/leeda/programing/test/LaMa_test_images:/input_images \
  -v /c/Users/leeda/programing/test/output:/output \
  lama-inpainting-env
```

*   `-it`: 대화형 터미널 세션을 유지합니다.
*   `--rm`: 컨테이너가 종료될 때 자동으로 삭제되도록 합니다.
*   `--gpus all`: NVIDIA GPU 사용 활성화
*   `--name lama-gpu-container`: 컨테이너 이름 지정
*   `-v /c/Users/leeda/programing/test/lama:/app`: 호스트의 `lama` 디렉토리를 컨테이너의 `/app` 디렉토리에 연결(마운트)합니다. 호스트에서 `lama` 디렉토리의 파일을 수정하면 컨테이너 내부에도 즉시 반영됩니다.
*   `-v /c/Users/leeda/programing/test/LaMa_test_images:/input_images`: 호스트의 `LaMa_test_images` 디렉토리를 컨테이너의 `/input_images` 디렉토리에 연결(마운트)합니다.
*   `-v /c/Users/leeda/programing/test/output:/output`: 호스트의 `output` 디렉토리를 컨테이너의 `/output` 디렉토리에 연결(마운트)합니다.
*   `lama-inpainting-env`: 실행할 이미지 이름입니다.

## 베이스 이미지 빌딩용 디렉토리 구조

```
/c:/Users/leeda/programing/test/
├── build_env/
│   ├── Dockerfile       # Docker 이미지 빌드 설정
│   └── requirements.txt # Python 의존성 목록
├── lama/
│   ├── bin/
│   │   ├── inference.py           # NumPy 기반 배치 추론 함수 (신규)
│   │   └── test_inference_docker.py # batch_inference 테스트 스크립트 (신규)
│   ├── configs/
│   ├── evaluation/
│   ├── models/
│   ├── saicinpainting/
│   └── training/
├── lama-fourier/
│   ├── best.ckpt
│   ├── config.yaml
│   └── last.ckpt      # 모델 파일 (Docker 이미지에 복사됨)
├── LaMa_test_images/            # 테스트 이미지/마스크
├── output/                      # 결과 저장 디렉토리
└── doc

```
## 도커 컨테이너 실행
docker run -it --rm --gpus all --name lama-gpu-container -v /c/Users/leeda/programing/test/lama:/app -v /c/Users/leeda/programing/test/LaMa_test_images:/input_images -v /c/Users/leeda/programing/test/output:/output lama-inpainting-gpu

## 최종 실행 환경 및 명령어 (GPU 지원, 2025-04-24 기준)
docker exec -w /app -e PYTHONPATH=/app -e TORCH_HOME=/app/.torch_cache -e USER=app_user lama-gpu-container python3 bin/test_inference_docker.py


## 나중에 가져다 쓰는 방법
lama 폴더만 때어다 가져가면 됨, 나머지는 그냥 환경에 관한 정보를 담고 있음

# 1. GPU 지원 컨테이너 시작 (백그라운드 실행)
# 역할: 필요한 볼륨(소스코드, 입력/출력 이미지)을 마운트하고 GPU를 사용하여 컨테이너를 백그라운드에서 실행합니다.
# lama-inpainting-gpu 이미지를 사용하며, tail 명령어로 컨테이너가 계속 실행되도록 합니다.

docker run -d --gpus all --name lama-gpu-final-test -v /c/Users/leeda/programing/test/lama:/app -v /c/Users/leeda/programing/test/LaMa_test_images:/input_images -v /c/Users/leeda/programing/test/output:/output lama-inpainting-gpu tail -f /dev/null

# 2. 컨테이너 내부에서 GPU 추론 실행
# 역할: 실행 중인 lama-gpu-final-test 컨테이너 내부에서 predict.py 스크립트를 실행합니다.
# 필요한 환경 변수(TORCH_HOME, PYTHONPATH)를 설정하고, 모델/입력/출력 경로와 GPU 사용(device=cuda)을 지정합니다.
# Hydra 로그 및 출력은 /output/.hydra_logs 에 저장하여 lama 소스 디렉토리를 깨끗하게 유지합니다.

docker exec lama-gpu-final-test bash -c "cd /app && export TORCH_HOME=/app && export PYTHONPATH=/app && python3 bin/predict.py model.path=/model indir=/input_images outdir=/output device=cuda hydra.run.dir=/output/.hydra_logs"


### 도커 베이스 이미지 정보
# Use PyTorch base image with CUDA 11.1 support for PyTorch 1.8.0
FROM pytorch/pytorch:1.8.0-cuda11.1-cudnn8-runtime

# Install build dependencies for C extensions and OpenCV runtime deps
# Base image is Ubuntu based. Need libgl for OpenCV.
RUN apt-get update && \
    apt-get install -y --no-install-recommends gcc build-essential libgl1-mesa-glx libglib2.0-0 && \
    rm -rf /var/lib/apt/lists/*

# Set the working directory
WORKDIR /app

# Copy requirements first to leverage Docker layer caching
# Dockerfile과 같은 디렉토리에 있는 requirements.txt를 복사
COPY build_env/requirements.txt .

# Install numpy first as it's a build dependency for some packages
# Install a specific numpy version compatible with pandas and scikit-image
RUN pip install --no-cache-dir numpy==1.23.5

# Install remaining dependencies from requirements file
# --no-cache-dir prevents caching downloads, reducing image size
RUN pip install --no-cache-dir -r requirements.txt

# Copy the specific model weights and config into /model directory
COPY lama-fourier /model/

# Create models subdir and move checkpoints into it to match expected structure
# Ensure the target directory exists and is empty, then move checkpoints
# RUN mkdir -p /model/models && rm -rf /model/models/* && mv /model/*.ckpt /model/models/

# Default command to run when the container starts (optional, useful for debugging)
CMD ["bash"] 
